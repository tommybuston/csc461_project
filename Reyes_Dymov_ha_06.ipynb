{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 6 (+ optional assignment) (due Dec 13th at 11:59pm)\n",
        "\n",
        "> **NOTE:** Although the assignment is due on Dec 13th, Gradescope will remain open and accepting late submissions with no penalties until Dec 20th at midnight.\n",
        "\n",
        "- This assignment covers SoftMax. Please refer to the class notes and corresponding Colabs on the course website for the required background.\n",
        "\n",
        "- This assignment is worth **200 points**. Half of the points are for Assignment 6 and the other half for the the **optional** assignment. After completing the solutions **you must submit a copy of this notebook (`.ipynb`), including all your answers**.\n",
        "\n",
        "- You can work alone or in pairs.  At the time of submission please **ONLY SUBMIT ONE FILE PER TEAM**.  Make sure to add all team members in your Gradescope submission.\n",
        "\n",
        "- You can't use `scikit-learn` for answering questions on this assignment.\n",
        "\n",
        ">  **Important**: Make sure **ALL cells are executed** before saving/downloading a copy of the notebook you will submit.\n",
        "\n",
        "> **Credit:** The starter code in this assignment is an adaptation of one of Stanford's CS231n assignments."
      ],
      "metadata": {
        "id": "n-uXkEHsUyO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ALL ANSWERS must be entered ONLY using the indicated spaces marked as TODOs, please DO NOT modify any of the starter code.\n",
        "> you must replace `pass` with your own code\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iUtgnhQTV4y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#####################################################################\n",
        "# TODO: 0 points                                                    #\n",
        "# Type your full names, using comments, one per line                #\n",
        "#####################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "#Joseph Reyes\n",
        "#Islam Dymov\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "metadata": {
        "id": "7auonYZrBUgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# imports and configuration\n",
        "import time\n",
        "from random import randrange\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# define constants for the size of all dataset partitions\n",
        "n_train = 45000\n",
        "n_valid = 1000\n",
        "n_test = 1000\n",
        "n_dev = 200"
      ],
      "metadata": {
        "id": "0Cy-wvSenjpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# downloading the CIFAR10 dataset\n",
        "(Xtr, Ytr), (Xte, Yte) = cifar10.load_data()\n",
        "\n",
        "# subsample the data for more efficient execution\n",
        "mask = list(range(n_train, n_train + n_valid))\n",
        "Xval, Yval = Xtr[mask], Ytr[mask]\n",
        "mask = list(range(n_train))\n",
        "Xtr, Ytr = Xtr[mask], Ytr[mask]\n",
        "mask = list(range(n_test))\n",
        "Xte, Yte = Xte[mask], Yte[mask]\n",
        "mask = np.random.choice(n_train, n_dev, replace=False)\n",
        "Xdev, Ydev = Xtr[mask], Ytr[mask]\n",
        "\n",
        "# reshaping images into flat rows\n",
        "Xtr = np.reshape(Xtr, (Xtr.shape[0], -1))\n",
        "Xval = np.reshape(Xval, (Xval.shape[0], -1))\n",
        "Xte = np.reshape(Xte, (Xte.shape[0], -1))\n",
        "Xdev = np.reshape(Xdev, (Xdev.shape[0], -1))\n",
        "\n",
        "# normalize the data: subtracting the mean image\n",
        "mean = np.mean(Xtr, axis = 0)\n",
        "Xtr = Xtr.astype(float, casting='safe', copy=False) - mean\n",
        "Xval = Xval.astype(float, casting='safe', copy=False) - mean\n",
        "Xte = Xte.astype(float, casting='safe', copy=False) - mean\n",
        "Xdev = Xdev.astype(float, casting='safe', copy=False) - mean\n",
        "\n",
        "# prepend column of 1s (bias)\n",
        "Xtr = np.concatenate((np.ones((Xtr.shape[0],1)),Xtr), axis=1)\n",
        "Xval = np.concatenate((np.ones((Xval.shape[0],1)),Xval), axis=1)\n",
        "Xte = np.concatenate((np.ones((Xte.shape[0],1)),Xte), axis=1)\n",
        "Xdev = np.concatenate((np.ones((Xdev.shape[0],1)),Xdev), axis=1)\n",
        "\n",
        "# sanity check\n",
        "print(f'Training data shapes:\\timages {Xtr.shape} labels {Ytr.shape}')\n",
        "print(f'Validation data shapes:\\timages {Xval.shape} labels {Yval.shape}')\n",
        "print(f'Test data shapes:\\timages {Xte.shape} labels {Yte.shape}')\n",
        "print(f'Development data shapes:\\timages {Xdev.shape} labels {Ydev.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAnWwWQxWfou",
        "outputId": "11be25d6-3e9a-4c1b-f934-0ab5e538f73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 5s 0us/step\n",
            "Training data shapes:\timages (45000, 3073) labels (45000, 1)\n",
            "Validation data shapes:\timages (1000, 3073) labels (1000, 1)\n",
            "Test data shapes:\timages (1000, 3073) labels (1000, 1)\n",
            "Development data shapes:\timages (200, 3073) labels (200, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_loss_naive(W, X, Y, reg):\n",
        "    \"\"\"\n",
        "    Softmax loss function, naive implementation (with loops)\n",
        "    Observations have dimension D, there are C classes, and we operate on \n",
        "    minibatches of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "      that X[i] has label c, where 0 <= c < C.\n",
        "    - lambda: regularization strength\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - cross-entropy loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    # initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape)\n",
        "    #############################################################################\n",
        "    # TODO: 50 points (assignment 6)                                            #\n",
        "    # Compute the softmax loss and its gradient using explicit loops.           #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget to         #\n",
        "    # implement the L2 regularization!                                          #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    #need to get the dimensions we will be working with\n",
        "    classes = W.shape[1]\n",
        "    train = X.shape[0]\n",
        "    dimensions = W.shape[0]\n",
        "\n",
        "    for i in range(train):\n",
        "      score_list = X[i,:].dot(W)\n",
        "      #scores_log = np.exp(-score_list.max()) #normalize for stability\n",
        "      scores_e = np.exp(score_list)\n",
        "      SoS = np.sum(scores_e)\n",
        "      score_probability = scores_e/SoS \n",
        "      #loss+= -np.log(score_probability[Y[i]])\n",
        "\n",
        "      for j in range(dimensions):\n",
        "          for k in range (classes):\n",
        "              if k == Y[i]:\n",
        "                dW[j,k] += X.T[j,i]*(score_probability[k] - 1)\n",
        "          \n",
        "              else:\n",
        "                dW[j,k] +=X.T[j,i] * score_probability[k]\n",
        "\n",
        "      loss+= -np.log(score_probability[Y[i]])\n",
        "    \n",
        "    loss /=train #divide regularized term by m\n",
        "    loss += 0.5*reg*np.sum(W**2) #divide by 2 and square\n",
        "\n",
        "    #update dW\n",
        "    dW /= train\n",
        "    dW += reg*W \n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "\n",
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "    \"\"\"\n",
        "    Inputs and outputs are the same as softmax_loss_naive.\n",
        "    \"\"\"\n",
        "    # initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(W.shape)\n",
        "    #############################################################################\n",
        "    # TODO: 50 points (assignment 6)                                            #\n",
        "    # Compute the softmax loss and its gradient using no explicit loops.        #\n",
        "    # Store the loss in loss and the gradient in dW.  If you are not careful    #\n",
        "    # here, it is easy to run into numeric instability. Don't forget to         #\n",
        "    # implement the L2 regularization!                                          #\n",
        "    #############################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    #need to get the dimensions we will be working with\n",
        "    train = X.shape[0]\n",
        "    y = y.reshape(train)\n",
        "    scorelist=np.dot(X,W)\n",
        "    expscorelist=np.exp(scorelist)\n",
        "    expsums=np.sum(expscorelist,axis=1, keepdims=True)\n",
        "    scoreprobabilities=expscorelist/expsums\n",
        "    truth=-np.log(scoreprobabilities[range(train),y])\n",
        "    loss=np.sum(truth)\n",
        "    #Ridge regularization \n",
        "    loss= (loss/train)+((reg)*np.sum(W**2))/2\n",
        "    dW=X.T.dot(scoreprobabilities)\n",
        "    dW = (dW/train)+ ((reg)*np.sum(W**2))/2\n",
        "\n",
        "\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return loss, dW"
      ],
      "metadata": {
        "id": "FKFfLg4wZRFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
        "    for i in range(num_checks):\n",
        "        ix = tuple([randrange(m) for m in x.shape])\n",
        "        oldval = x[ix]\n",
        "        x[ix] = oldval + h  # increment by h\n",
        "        fxph = f(x)  # evaluate f(x + h)\n",
        "        x[ix] = oldval - h  # increment by h\n",
        "        fxmh = f(x)  # evaluate f(x - h)\n",
        "        x[ix] = oldval  # reset\n",
        "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
        "        grad_analytic = analytic_grad[ix]\n",
        "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
        "        print(f'numerical: {grad_numerical} analytic: {grad_analytic}, relative error: {rel_error}')"
      ],
      "metadata": {
        "id": "NKXxSkA70GDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# generating a random weight matrix and use it to compute the loss\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, Xdev, Ydev, 0.0)\n",
        "\n",
        "# sanity check, our loss should be something close to -log(0.1).\n",
        "print(f'loss: {loss}, compared to sanity check {-np.log(0.1)}\\n')\n",
        "\n",
        "# using numeric gradient checking as a debugging tool.\n",
        "# the numeric gradient should be close to the analytic gradient.\n",
        "f = lambda w: softmax_loss_naive(w, Xdev, Ydev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# doing another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, Xdev, Ydev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, Xdev, Ydev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMpn2yFzaVa-",
        "outputId": "69375b06-4882-4773-952d-c95902fa9b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: [2.38801527], compared to sanity check 2.3025850929940455\n",
            "\n",
            "numerical: [0.23978913] analytic: 0.2397891583901038, relative error: [6.76318286e-08]\n",
            "numerical: [1.42659315] analytic: 1.4265931701953039, relative error: [5.78234457e-09]\n",
            "numerical: [-1.34135702] analytic: -1.3413570859029003, relative error: [2.59018736e-08]\n",
            "numerical: [-0.9618596] analytic: -0.9618596550289213, relative error: [2.91644555e-08]\n",
            "numerical: [-1.54930296] analytic: -1.5493029878472, relative error: [1.01005986e-08]\n",
            "numerical: [-4.29555286] analytic: -4.295552991509899, relative error: [1.47454358e-08]\n",
            "numerical: [-1.2606551] analytic: -1.260655158210656, relative error: [2.41092806e-08]\n",
            "numerical: [1.5272853] analytic: 1.5272853748866395, relative error: [2.30528418e-08]\n",
            "numerical: [3.45149598] analytic: 3.4514958892037537, relative error: [1.34845269e-08]\n",
            "numerical: [2.73607488] analytic: 2.7360748249676097, relative error: [1.02789425e-08]\n",
            "numerical: [0.85745468] analytic: 0.8574545858411761, relative error: [5.72968259e-08]\n",
            "numerical: [1.66681732] analytic: 1.6668173083625661, relative error: [4.47359367e-09]\n",
            "numerical: [-0.31569674] analytic: -0.3156968176781248, relative error: [1.18393553e-07]\n",
            "numerical: [-0.07751158] analytic: -0.0775116518460832, relative error: [4.55153145e-07]\n",
            "numerical: [-0.17152923] analytic: -0.17152924858337068, relative error: [4.10377277e-08]\n",
            "numerical: [-2.39133886] analytic: -2.391338768977782, relative error: [1.85064363e-08]\n",
            "numerical: [-1.57913707] analytic: -1.579137045700979, relative error: [8.82435565e-09]\n",
            "numerical: [-0.03004276] analytic: -0.030042846582697492, relative error: [1.49435603e-06]\n",
            "numerical: [-0.78858607] analytic: -0.788586077643572, relative error: [2.00270995e-09]\n",
            "numerical: [-1.1335615] analytic: -1.1335614134453487, relative error: [3.62192173e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# checking that the vectorized version in softmax_loss_vectorized\n",
        "# computes the same results. The vectorized version should be much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, Xdev, Ydev, 0.000005)\n",
        "toc = time.time()\n",
        "print(f'naive loss: {loss_naive} computed in {toc-tic} sec')\n",
        "\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, Xdev, Ydev, 0.000005)\n",
        "toc = time.time()\n",
        "print(f'vectorized loss: {loss_vectorized} computed in {toc--tic} sec')\n",
        "\n",
        "# use the Frobenius norm to compare the two versions of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print(f'loss difference: {np.abs(loss_naive-loss_vectorized)}')\n",
        "print(f'gradient difference: {grad_difference}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6eXZMVSb8oG",
        "outputId": "cb31fec2-9f12-4744-ff42-2df1bd5598fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "naive loss: [2.34256238] computed in 26.34746813774109 sec\n",
            "vectorized loss: 2.3425623794010395 computed in 3343131842.7717323 sec\n",
            "loss difference: [0.]\n",
            "gradient difference: 429.6270889875715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you are not working on the optional assignment, stop here"
      ],
      "metadata": {
        "id": "b49ig19t_tFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(object):\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(self, X, Y, lr=1e-3, reg=1e-5, n_iters=100, b_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - Y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - lr: (float) learning rate for optimization.\n",
        "        - reg: (float) regularization strength.\n",
        "        - n_iters: (integer) number of steps to take when optimizing\n",
        "        - b_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = (np.max(Y) + 1)  \n",
        "        if self.W is None: # initialize weights\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(n_iters):\n",
        "            X_batch = None\n",
        "            Y_batch = None\n",
        "            #########################################################################\n",
        "            # TODO: 20 points (optional)                                            #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # Y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n",
        "            # and Y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            randomsamples=np.random.choice(num_train,batch_size,replace=False)\n",
        "            X_batch=X[randomsamples]\n",
        "            Y_batch=y[randomsamples]\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, Y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO: 20 points (optional)                                            #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            self.W-=grad*learning_rate\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print(f'iteration {it} / {n_iters}: loss {loss}')\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ########################################################################\n",
        "        # TODO: 30 points                                                      #\n",
        "        # Implement this method. Store the predicted labels in y_pred.         #\n",
        "        ########################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        scores=X.dot(W)\n",
        "        y_pred=np.argmax(X.dot(self.W))\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, bX, bY, reg):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "        - bX: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - bY: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "        - reg: (float) regularization strength.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        return softmax_loss_vectorized(self.W, bX, bY, reg)"
      ],
      "metadata": {
        "id": "W61FTAjaq7du"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, regularization_strength) to tuples of the form\n",
        "# (training_accuracy, validation_accuracy)\n",
        "results = {}\n",
        "# highest validation accuracy that we have seen so far.\n",
        "best_val = -1\n",
        "# Softmax object that achieved the highest validation rate.\n",
        "best_softmax = None\n",
        "\n",
        "################################################################################\n",
        "# TODO:  30 points (optional)                                                  #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# Save the best trained softmax classifer object in best_softmax.              #\n",
        "#                                                                              #\n",
        "# For each combination of hyperparameters, train a softmax classifier on the   #\n",
        "# training set, compute its accuracy on the training and validation sets, and  #\n",
        "# store these numbers in the results dictionary. In addition, store the best   #\n",
        "# validation accuracy in best_val and the classifer that achieves this         #\n",
        "# accuracy in best_softmax.                                                    #\n",
        "#                                                                              #\n",
        "# Hint: You should use a small number of iterations as you develop the         #\n",
        "# validation code; once you are confident that your validation code works, you #\n",
        "# should rerun the validation code using a larger number.                      #\n",
        "#                                                                              #\n",
        "# Note: you may see runtime/overflow warnings during hyper-parameter search. \n",
        "# This may be caused by extreme values, and is not a bug.\n",
        "################################################################################\n",
        "\n",
        "# provided as a reference. You may want (should) to change these hyperparameters\n",
        "learning_rates = [1e-7, 5e-7]\n",
        "regularization_strengths = [2.5e4, 5e4]\n",
        "\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "pass\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    tr_acc, val_acc = results[(lr, reg)]\n",
        "    print(f'lr {lr} reg {reg} train accuracy: {tr_acc} val accuracy: {val_acc}')\n",
        "print(f'best validation accuracy achieved: {best_val}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYy-BIMafQmf",
        "outputId": "c1267db8-d4ef-4680-bee0-236f90925f4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best validation accuracy achieved: -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(Xte)\n",
        "test_accuracy = np.mean(Yte == y_test_pred)\n",
        "print(f'softmax on raw pixels final test set accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "9adGNSdXfs3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################\n",
        "# do not modify this cell\n",
        "###################################################\n",
        "\n",
        "# visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    # rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "metadata": {
        "id": "WgfDMgsNguF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uilkfUHRAkml"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}